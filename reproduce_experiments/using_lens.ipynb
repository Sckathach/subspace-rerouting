{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the main tools \n",
    "\n",
    "This project currently uses the Transformer Lens library, as it makes it easy and straightforward to use PyTorch Hooks. \n",
    "- Main page: <https://transformerlensorg.github.io/TransformerLens/>\n",
    "- Getting started: <https://transformerlensorg.github.io/TransformerLens/content/getting_started.html>\n",
    "- (Excellents) tutorials: <https://transformerlensorg.github.io/TransformerLens/content/tutorials.html>\n",
    "\n",
    "I highly recommend the extraordinary course ARENA, to explore the techniques used in this paper (DLA, Attribution patching, etc.):\n",
    "- Website: <https://www.arena.education/> \n",
    "- Course: <https://arena-chapter1-transformer-interp.streamlit.app/>\n",
    "\n",
    "This notebooks aims to give you the necessary part to understand and use the code of the paper. \n",
    "\n",
    "## Lens \n",
    "I made a class named `Lens` in `ssr/lens.py`, which has three main functions: \n",
    "- Allow quick load of preconfigured LLMs \n",
    "- Easy way to apply the correct chat template\n",
    "- Allow batched CPU scans \n",
    "\n",
    "The main SSR algorithm (`ssr/core.py`) only needs the `Lens` class to apply the chat template. I'll modify that in the future, so the core algorithm does not depend on my custom `Lens` class, but only on Transformer Lens. \n",
    "\n",
    "I'll present here the three main functions of my custom `Lens` class. \n",
    "\n",
    "### 1. Quick load of preconfigured LLMs\n",
    "\n",
    "I used four main LLMs in this work: \n",
    "- Gemma 2 2b: `gemma2_2b`, <https://huggingface.co/google/gemma-2-2b-it> (gated)\n",
    "- Llama 3.2 1b: `llama3.2_1b`, <https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct> (gated)\n",
    "- Llama 3.2 3b: `llama3.2_3b`, <https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct> (gated)\n",
    "- Qwen 2.5 1.5b: `qwen2.5_1.5b`, <https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct>\n",
    "\n",
    "As the chat templates may vary depending on the versions, I picked the official jinja template for each model, put in `ssr/templates/*`, and sticked to these ones for every experiments. \n",
    "\n",
    "For the rest of the configuration, I put everything in the `models.toml` file, at the root of the project. \n",
    "\n",
    "To get the default config for a LLM, first make sure the `models.toml` is at the root of the folder, otherwise modify the `MODELS_PATH` value in the environment variables (`.env`). Then, you can access the config with the `model_info` function from `lens.py`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'chat_template'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2.jinja2'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'lm_studio'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama-3.2-1b-instruct'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'meta-llama/Llama-3.2-1B-Instruct'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'other_names'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Llama-3.2-1B-Instruct'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'restricted_tokens'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'128000-128255'</span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'chat_template'\u001b[0m: \u001b[32m'llama3.2.jinja2'\u001b[0m,\n",
       "    \u001b[32m'lm_studio'\u001b[0m: \u001b[32m'llama-3.2-1b-instruct'\u001b[0m,\n",
       "    \u001b[32m'model_name'\u001b[0m: \u001b[32m'meta-llama/Llama-3.2-1B-Instruct'\u001b[0m,\n",
       "    \u001b[32m'other_names'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'Llama-3.2-1B-Instruct'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'restricted_tokens'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'128000-128255'\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print \n",
    "from ssr.lens import model_info\n",
    "\n",
    "print(model_info(\"llama3.2_1b\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "{\n",
    "    'chat_template': 'llama3.2.jinja2',                 # location of the chat template file (ssr/template/llama3.2.jinja2)\n",
    "    'lm_studio': 'llama-3.2-1b-instruct',               # name of the model in LM Studio\n",
    "    'model_name': 'meta-llama/Llama-3.2-1B-Instruct',   # name of the model in Transformer Lens \n",
    "    'other_names': ['Llama-3.2-1B-Instruct'],           # other names (needed for SAELens for instance)\n",
    "    'restricted_tokens': ['128000-128255']              # range of restricted tokens (ie: we don't want to get adversarial candidates with <eos> in the base scenario) \n",
    "}\n",
    "```\n",
    "\n",
    "The LLM will be instancied as: \n",
    "```python\n",
    "model = tl.HookedTransformer.from_pretrained(\n",
    "    model_name=kwargs[\"model_name\"],\n",
    "    device=device,\n",
    "    dtype=\"float16\",\n",
    "    center_unembed=kwargs.get(\"center_unembed\", True),\n",
    "    center_writing_weights=kwargs.get(\"center_writing_weights\", True),\n",
    "    fold_ln=kwargs.get(\"fold_ln\", True),\n",
    ")\n",
    "\n",
    "model.tokenizer.chat_template = chat_template      \n",
    "model.tokenizer.padding_side = padding_side             \n",
    "model.tokenizer.pad_token = pad_token\n",
    "``` \n",
    "\n",
    "The `chat_template` argument can either be a path (end with `.jinja2`), or the str version of the jinja chat template directly. \n",
    "\n",
    "This allows us to load common LLMs quickly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.2-1B-Instruct into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from ssr.lens import Lens \n",
    "\n",
    "lens = Lens.from_config(\"llama3.2_1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Lens` object is simply a class with a property model, which is the Transformer Lens model, and utility methods. To access the Transformer Lens model simply use `lens.model`. Hence the configuration can be printed with: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HookedTransformerConfig:\n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'NTK_by_parts_factor'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32.0</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'NTK_by_parts_high_freq_factor'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.0</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'NTK_by_parts_low_freq_factor'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'act_fn'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'silu'</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'attention_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'causal'</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'attn_only'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'attn_scale'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.0</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'attn_scores_soft_cap'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.0</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'attn_types'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'checkpoint_index'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'checkpoint_label_type'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'checkpoint_value'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'d_head'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'d_mlp'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8192</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'d_model'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'d_vocab'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128256</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'d_vocab_out'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128256</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'decoder_start_token_id'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'default_prepend_bos'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'device'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'cuda:0'</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'dtype'</span>: torch.float16,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'eps'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-05</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'experts_per_token'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'final_rms'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'from_checkpoint'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'gated_mlp'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'init_mode'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gpt2'</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'init_weights'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'initializer_range'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.017677669529663688</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'load_in_4bit'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Llama-3.2-1B-Instruct'</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'n_ctx'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'n_devices'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'n_heads'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'n_key_value_heads'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'n_layers'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'n_params'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1073741824</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'normalization_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'RMS'</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'num_experts'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'original_architecture'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'LlamaForCausalLM'</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'output_logits_soft_cap'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.0</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'parallel_attn_mlp'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'positional_embedding_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'rotary'</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'post_embedding_ln'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'relative_attention_max_distance'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'relative_attention_num_buckets'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'rotary_adjacent_pairs'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'rotary_base'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">500000.0</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'rotary_dim'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'scale_attn_by_inverse_layer_idx'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'seed'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'tie_word_embeddings'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'tokenizer_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'meta-llama/Llama-3.2-1B-Instruct'</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'tokenizer_prepends_bos'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'trust_remote_code'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'ungroup_grouped_query_attention'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'use_NTK_by_parts_rope'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'use_attn_in'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'use_attn_result'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'use_attn_scale'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'use_hook_mlp_in'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'use_hook_tokens'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'use_local_attn'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'use_normalization_before_and_after'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'use_split_qkv_input'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'window_size'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "HookedTransformerConfig:\n",
       "\u001b[1m{\u001b[0m\u001b[32m'NTK_by_parts_factor'\u001b[0m: \u001b[1;36m32.0\u001b[0m,\n",
       " \u001b[32m'NTK_by_parts_high_freq_factor'\u001b[0m: \u001b[1;36m4.0\u001b[0m,\n",
       " \u001b[32m'NTK_by_parts_low_freq_factor'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       " \u001b[32m'act_fn'\u001b[0m: \u001b[32m'silu'\u001b[0m,\n",
       " \u001b[32m'attention_dir'\u001b[0m: \u001b[32m'causal'\u001b[0m,\n",
       " \u001b[32m'attn_only'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       " \u001b[32m'attn_scale'\u001b[0m: \u001b[1;36m8.0\u001b[0m,\n",
       " \u001b[32m'attn_scores_soft_cap'\u001b[0m: \u001b[1;36m-1.0\u001b[0m,\n",
       " \u001b[32m'attn_types'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       " \u001b[32m'checkpoint_index'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       " \u001b[32m'checkpoint_label_type'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       " \u001b[32m'checkpoint_value'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       " \u001b[32m'd_head'\u001b[0m: \u001b[1;36m64\u001b[0m,\n",
       " \u001b[32m'd_mlp'\u001b[0m: \u001b[1;36m8192\u001b[0m,\n",
       " \u001b[32m'd_model'\u001b[0m: \u001b[1;36m2048\u001b[0m,\n",
       " \u001b[32m'd_vocab'\u001b[0m: \u001b[1;36m128256\u001b[0m,\n",
       " \u001b[32m'd_vocab_out'\u001b[0m: \u001b[1;36m128256\u001b[0m,\n",
       " \u001b[32m'decoder_start_token_id'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       " \u001b[32m'default_prepend_bos'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       " \u001b[32m'device'\u001b[0m: \u001b[32m'cuda:0'\u001b[0m,\n",
       " \u001b[32m'dtype'\u001b[0m: torch.float16,\n",
       " \u001b[32m'eps'\u001b[0m: \u001b[1;36m1e-05\u001b[0m,\n",
       " \u001b[32m'experts_per_token'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       " \u001b[32m'final_rms'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       " \u001b[32m'from_checkpoint'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       " \u001b[32m'gated_mlp'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       " \u001b[32m'init_mode'\u001b[0m: \u001b[32m'gpt2'\u001b[0m,\n",
       " \u001b[32m'init_weights'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       " \u001b[32m'initializer_range'\u001b[0m: \u001b[1;36m0.017677669529663688\u001b[0m,\n",
       " \u001b[32m'load_in_4bit'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       " \u001b[32m'model_name'\u001b[0m: \u001b[32m'Llama-3.2-1B-Instruct'\u001b[0m,\n",
       " \u001b[32m'n_ctx'\u001b[0m: \u001b[1;36m2048\u001b[0m,\n",
       " \u001b[32m'n_devices'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       " \u001b[32m'n_heads'\u001b[0m: \u001b[1;36m32\u001b[0m,\n",
       " \u001b[32m'n_key_value_heads'\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
       " \u001b[32m'n_layers'\u001b[0m: \u001b[1;36m16\u001b[0m,\n",
       " \u001b[32m'n_params'\u001b[0m: \u001b[1;36m1073741824\u001b[0m,\n",
       " \u001b[32m'normalization_type'\u001b[0m: \u001b[32m'RMS'\u001b[0m,\n",
       " \u001b[32m'num_experts'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       " \u001b[32m'original_architecture'\u001b[0m: \u001b[32m'LlamaForCausalLM'\u001b[0m,\n",
       " \u001b[32m'output_logits_soft_cap'\u001b[0m: \u001b[1;36m-1.0\u001b[0m,\n",
       " \u001b[32m'parallel_attn_mlp'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       " \u001b[32m'positional_embedding_type'\u001b[0m: \u001b[32m'rotary'\u001b[0m,\n",
       " \u001b[32m'post_embedding_ln'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       " \u001b[32m'relative_attention_max_distance'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       " \u001b[32m'relative_attention_num_buckets'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       " \u001b[32m'rotary_adjacent_pairs'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       " \u001b[32m'rotary_base'\u001b[0m: \u001b[1;36m500000.0\u001b[0m,\n",
       " \u001b[32m'rotary_dim'\u001b[0m: \u001b[1;36m64\u001b[0m,\n",
       " \u001b[32m'scale_attn_by_inverse_layer_idx'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       " \u001b[32m'seed'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       " \u001b[32m'tie_word_embeddings'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       " \u001b[32m'tokenizer_name'\u001b[0m: \u001b[32m'meta-llama/Llama-3.2-1B-Instruct'\u001b[0m,\n",
       " \u001b[32m'tokenizer_prepends_bos'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       " \u001b[32m'trust_remote_code'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       " \u001b[32m'ungroup_grouped_query_attention'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       " \u001b[32m'use_NTK_by_parts_rope'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       " \u001b[32m'use_attn_in'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       " \u001b[32m'use_attn_result'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       " \u001b[32m'use_attn_scale'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       " \u001b[32m'use_hook_mlp_in'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       " \u001b[32m'use_hook_tokens'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       " \u001b[32m'use_local_attn'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       " \u001b[32m'use_normalization_before_and_after'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       " \u001b[32m'use_split_qkv_input'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       " \u001b[32m'window_size'\u001b[0m: \u001b[3;35mNone\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(lens.model.cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Applying the chat template\n",
    "\n",
    "The `apply_chat_template` method is a restriction of the more general `tokenizer.apply_chat_template` from Hugging Face. The signature of the function is: \n",
    "\n",
    "```python\n",
    "def apply_chat_template(\n",
    "    self,\n",
    "    messages: str | List[Dict[str, str]],\n",
    "    tokenize: Literal[True] | Literal[False] = False,\n",
    "    add_generation_prompt: bool = True,\n",
    "    system_message: Optional[str] = None,\n",
    "    role: str = \"user\",\n",
    "    **kwargs,\n",
    ") -> str | BatchEncoding:\n",
    "```\n",
    "\n",
    "Example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">|begin_of_text|</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Super cool!&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95m|begin_of_text|\u001b[0m\u001b[39m><|start_header_id|>user<|end_header_id|>\u001b[0m\n",
       "\n",
       "\u001b[39mSuper cool!<|eot_id|><|start_header_id|>assistant<|end_header_id|\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(lens.apply_chat_template(\"Super cool!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a system message, this gives: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">|begin_of_text|</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">You are a helpful assistant.&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Super cool!&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95m|begin_of_text|\u001b[0m\u001b[39m><|start_header_id|>system<|end_header_id|>\u001b[0m\n",
       "\n",
       "\u001b[39mYou are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\u001b[0m\n",
       "\n",
       "\u001b[39mSuper cool!<|eot_id|><|start_header_id|>assistant<|end_header_id|\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(lens.apply_chat_template(\"Super cool!\", system_message=\"You are a helpful assistant.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Batch scan to CPU\n",
    "\n",
    "When using Transformer Lens, the first major issue is the GPU needed to perform any task. For instance, to run a forward pass on a dataset and cache the activations on the CPU, one usually first run the forward pass, store all the needed intermediate activations in the `ActivationCache` object, then uses the `.to(\"cpu\")` method to transfer it to the CPU. However, in practice, the GPU might be full long before the end of the forward pass. Furthermore, as there is no protection to OOM errors, when working on a jupyter notebook, every OOM error means the full notebook has to be reloaded. \n",
    "\n",
    "To overcome these problems, I implemented the `auto_scan` method, which will:\n",
    "- Store each batch's activations to the CPU before processing the next batch (`batch_scan_to_cpu`)\n",
    "- Catch OOM errors and reduce the batch size if necessary (`find_executable_batch_size`)\n",
    "\n",
    "This leads to the following operation being possible: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5200</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m5200\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Decreasing batch size to: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2600</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Decreasing batch size to: \u001b[1;36m2600\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Decreasing batch size to: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1300</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Decreasing batch size to: \u001b[1;36m1300\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Decreasing batch size to: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">650</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Decreasing batch size to: \u001b[1;36m650\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Decreasing batch size to: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">325</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Decreasing batch size to: \u001b[1;36m325\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Decreasing batch size to: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">162</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Decreasing batch size to: \u001b[1;36m162\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/33 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Decreasing batch size to: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">81</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Decreasing batch size to: \u001b[1;36m81\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 12/65 [00:34<03:42,  4.19s/it]"
     ]
    }
   ],
   "source": [
    "from ssr.datasets import load_dataset\n",
    "\n",
    "hf, _ = load_dataset(\"adv\", max_samples=520)\n",
    "\n",
    "print(len(hf))\n",
    "\n",
    "hf_scan = lens.auto_scan(hf, padding=True)  # no chat template here /!\\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
